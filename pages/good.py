import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import folium
from folium.plugins import MarkerCluster
import os
from io import BytesIO
from collections import deque
from math import log2
from statsmodels.api import OLS, add_constant

# ê²½ë¡œ ì„¤ì •
def get_path(filename):
    return os.path.join(os.path.dirname(__file__), filename)

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_data():
    df_power = pd.read_csv(get_path("power_by_region.csv"))
    df_temp = pd.read_csv(get_path("temperature_by_region.csv"))
    df_hourly = pd.read_csv(get_path("hourly_power.csv"))
    df_sdg = pd.read_csv(get_path("7-1-1.csv"))
    return df_power, df_temp, df_hourly, df_sdg

df_power, df_temp, df_hourly, df_sdg = load_data()

# ìƒ¤ë…¼ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
def compute_entropy(group):
    counts = group['Usage'].value_counts()
    prob = counts / counts.sum()
    return -np.sum([p * log2(p) for p in prob if p > 0])

df_power.rename(columns={
    'ì‹œë„': 'Province',
    'ì‹œêµ°êµ¬': 'City',
    'ì‚¬ìš©ëŸ‰': 'Usage'
}, inplace=True)

entropy_df = df_power.groupby(['Province', 'City']).apply(compute_entropy).reset_index(name='Entropy')

# ë³€ë¶„ë²• ê¸°ë°˜ ë¶„ì„ (ê°€ì¤‘í•© ìµœì†Œí™”)
def variational_minimize(data):
    return data['Usage'].mean() + data['Usage'].std() * 0.5

optimized_score = variational_minimize(df_power)

# íƒìƒ‰ ë° ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ - ë²„ë¸” ì •ë ¬ ì˜ˆì‹œ
def bubble_sort_regions(df):
    items = df.groupby("City")["Usage"].sum().reset_index().values.tolist()
    n = len(items)
    for i in range(n):
        for j in range(0, n - i - 1):
            if items[j][1] < items[j + 1][1]:
                items[j], items[j + 1] = items[j + 1], items[j]
    return pd.DataFrame(items, columns=["City", "TotalUsage"])

sorted_df = bubble_sort_regions(df_power)

# í / ìŠ¤íƒ ì˜ˆì‹œ
power_queue = deque(df_hourly['Usage'].head(10))
power_stack = list(df_hourly['Usage'].tail(10))

# íšŒê·€ ë¶„ì„: ì˜¨ë„ vs ì „ë ¥ ì‚¬ìš©ëŸ‰
avg_temp = df_temp.groupby("Region")["í‰ê· ê¸°ì˜¨ê°’"].mean().reset_index()
avg_usage = df_power.groupby("City")["Usage"].mean().reset_index()
df_merged = pd.merge(avg_temp, avg_usage, left_on="Region", right_on="City")

fig1 = px.scatter(df_merged, x='í‰ê· ê¸°ì˜¨ê°’', y='Usage', trendline='ols', title='Regression: Temperature vs Power')

# ì§€ë„ ê¸°ë°˜ ì „ë ¥ ë°°ì „ íŠ¸ë¦¬ ì‹œê°í™”
m = folium.Map(location=[36.5, 127.5], zoom_start=7)
marker_cluster = MarkerCluster().add_to(m)
for _, row in df_power.iterrows():
    try:
        if not (np.isnan(row['lat']) or np.isnan(row['lon'])):
            folium.Marker(
                location=[row['lat'], row['lon']],
                popup=f"{row['Province']} > {row['City']}",
                icon=folium.Icon(color='blue', icon='bolt', prefix='fa')
            ).add_to(marker_cluster)
    except:
        continue
m.save("/mnt/data/distribution_tree_map.html")

# Streamlit UI
st.title("Power Usage Analysis with Algorithm & Entropy")
st.plotly_chart(fig1)
st.write("\n### ğŸ” Sorted Regions by Usage (Bubble Sort)")
st.dataframe(sorted_df)
st.write("\n### âš™ï¸ Entropy by Region")
st.dataframe(entropy_df)
st.write("\n### âœ… Optimized Score (Variational Calculus)")
st.metric("Optimized Regional Score", f"{optimized_score:.2f}")
st.write("\n### ğŸ“ Power Distribution Map")
st.components.v1.iframe("/mnt/data/distribution_tree_map.html", height=500)
